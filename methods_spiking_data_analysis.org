* Dimensionality reduction
  We want to reduce the dimensionality of output space, reducing it to
  a discrete set by constructing a /decoder/ that can assign a label
  between 1 and N=1024 to any spike count vector ("output data
  point").

* Tessellating output space
  There are many ways of doing this: for example, we could just take
  all our data and apply some clustering algorithm (like k-means or
  some type of hierarchical clustering) to it, dividing it directly in
  N subsets. But this wouldn't be very appropriate in our case as it
  would be a batch operation, requiring knowledge of the entire
  dataset all at once, and wouldn't fit too well with the idea that
  downstream systems should - perhaps after an appropriate learning
  period - be able to classify each possible input "on the fly", as it
  is presented. Instead, we chose a much simpler approach: we divide
  the output space in N regions that should be non-overlapping and
  whose union should cover the entirety of the space. In other words,
  we make a tessellation for the output space with 1024 domains. Data
  points are then classified by which region they belong to (note
  that, by our definition, each point in output space must belong to
  one and only one region).

* Voronoi Tessellation
  Again, there are obviously many (many!) ways of tessellating the
  space, and we went for a very simple option: the /Voronoi/, or
  nearest-neighbor, tessellation. This consists in somehow choosing N
  special points (let's call them "centroids": $c_1,\ldots,c_N$ ) in
  the space, and delimiting the domains such that
  \begin{equation}
  d(x,c_k) \leq d(x,c_i) \quad\forall i\neq k \quad.
  \end{equation}
  This is equivalent to saying that we classify output data points by
  which centroid they're closest to.

* k-means
  But how do we choose the centroids? Here's what we do:
  - we generate a /training/ dataset, which should have the same
    statistics that the actual "/testing/" dataset we're trying to
    "decode";
  - we divide it in N cluster using the k-means algorithm (Lloyd,
    1982);
  - we select the centres of the clusters to be the centroids of our
    Voronoi tessellation.
  We ended up using a training set of the same size of the testing set
  (30 repetitions per patterns in each), but in principle there is no
  reason why the two datasets shouldn't differ in size. In fact, we
  test what happens as we change the size of the training dataset in
  figure S4D (top).
** Avoiding local minima
   k-means is a very simple but powerful algorithm, but it does have
   the tendency of getting stuck in local minima of "inertia" (a
   measure of goodness of clustering calculated as the sum of the
   intracluster variance); in other words, it can get stuck in
   suboptimal solutions. The algorithm itself is almost deterministic
   (with the exception of when it needs to assign a data points to one
   of two clusters whose centres are equidistant from the point), but
   its final result depends on a random initial assignment of each
   point to one of the N clusters. So, to avoid suboptimal solutions,
   we simply run the algorithm a number of times, each time
   re-initialising it from a different random assignment of points to
   clusters.
** /k-means++/
   /k-means++/ (Arthur and Vassilvitskii, 2007) is a procedure to make
   this assignment a little less random, to accelerate the convergence
   of the main optimisation.
** minibatch k-means
   /minibatch k-means/ is a variant of k-means which - without going
   in too much details - trades clustering accuracy for speed. At some
   point I was using this, as not having to wait one day for each
   analysis run meant that I was able to iterate faster on the design
   of the data analysis system, but I decided not to use it "in
   production" as the results it was giving (at least for the
   combinations of settings I explored) were really poor and noisy (ie
   variable from run to run on the same data) compared to full
   k-means.

* Undersampling bias
  Again, I'm not gonna explain the whole thing here, but this
  essentially happens when we don't have enough data points to
  estimate entropies for our NxN communication channel (Treves and
  Panzeri, 1995). This problem can be mitigated by using smart bias
  correction methods, by increasing the number of data points used for
  estimating the entropies, or both. This is what we show in figure
  S4D, bottom panel: the estimated information goes down, as bias is
  suppressed, if we adopt a bias correction method (we use the method
  described in Strong 1998, which we have concluded being very good
  after comparing with more recent alternatives, as it's implemented
  in the /pyentropy/ package (Ince et al., 2009)) or if we increase
  the size of the testing dataset.
