* Is it possible to use my simulations to support the main claim made in Guy's paper?
** Meeting with Angus and Guy <2013-11-19 Tue>
*** Points for discussion
1. what cell model? should I use my new cell model population? in this
   case, what about the single-cell-level disorder? given that's
   impossible to treat this by annealing (ie averaging over it), at
   which point should I quench it (ie fix network configuration)? An
   alternative is to use the average model from my population.
2. I also need to remember to update the value of the GABA reversal
   potential with the figure from Seja2012.
3. Speaking of quenched disorder, what did Guy do for the
   connection-driven disorder?
4. I guess we're going to use my new model for the mf->grc
   synapse.. this means using jLEMS (ie suicide) unless we get the
   network simulation export function to run. Or do I? probably not,
   if the network is still described with neuroConstruct.
5. What exactly do we mean with "testing with rate code"? if it's only
   rate in-rate out, we don't need to use Conor's metric (or more
   precisely, we don't need a Van Rossum-type metric), as we can just
   use spike counts.
6. How to measure sparsity in the rate-coded case? An interesting idea
   would be to use the Gini index, but this is a measure of inequality
   and would give sparsity=1 in the case of a silent network. If by
   sparsity we really mean "global activation level", I can just use
   the average firing rate of the network, but in that case I would
   still have to normalize it somehow to compare it to the firing rate
   of the inputs (to impose the sparsifying condition p(GC)<p(MF)).
7. I guess the best thing to do would be to check if my simulations
   agree with Guy's prediction in a low entropy limit, or in general
   to compare for eg the 4000-pattern case (might help to consider
   short time windows and spike counting metric). Otherwise, it's
   going to be impossible to do all of the required simulations.
8. One thing I still need to do is to implement Guy's connectivity
   rules (or maybe not, in case we want to compare the numerical
   results with the random bipartite graph).
9. what about doing a frequency-response curve of the grc model to
   test the 30ms integration window hypothesis? As an RC circuit, my
   average grc model has a time constant of about 6ms (so no problem
   on that front), and the longest decay time constant in my synaptic
   models is 121ms for the second component of the NMDAr-mediated
   synapse. But this component is less than 1/6th in amplitude than
   the first component, which has a time constant of 13.5ms, so
   overall I might get lucky and get away with considering a 50ms
   integration window.
10. for the time windowing side of things, it might be possible to just
    do the same simulations with synchronised and desynchronised Golgi
    input, and see what effect that has on the rate-coded information
    flow. We don’t expect it to have /any/ effect, but that should help
    address the reviewer’s concerns.
11. It might even be possible, even though probably outside the scope
    of the work, make an /analytical/ rate-code extension to the
    existing work. Moreno-Bote et al (2006) estimate the firing rate
    and the Fano factor for the spike count in the case of a leaky
    integrate-and-fire neuron with Poisson input and synaptic time
    constants longer than the membrane time constant. This could in
    theory enable us to calculate mean and variance for the spike count
    of a neuron with any given input, and so, by hypothesising a
    Gaussian distribution in spike count, the full conditional
    probability distribution of the spike counts for all the cells in a
    network given the input rates and the connectivity matrix. from
    here one should be able to calculate the MI, as this would look
    like some sort of multidimensional Gaussian channel. This is
    analogous to what Guy did for the explicit calculation of entropy
    in the "small" case with 4000 patterns. Anyway, this approach would
    break down near the refractory limit.
12. How are we going to deal with the effect of STP in my GoC->GrC
    synaptic model? One could hardcode a dependency of the Golgi
    firing rate on the p(MF) parameter, but because of depression, the
    time-averaged amount of inhibitory conductance doesn't scale
    linearly with the Golgi firing rate. Initially we will just use
    a varying tonic inhibition as a thresholding mechanism, and once
    we are suer we can get that working we will move on to using
    inhibitory synaptic input. Note that the levels of tonic
    conductance will be tuned depending on their thresholding effect,
    and not on their biological realism. This will probably mean
    choosing unrealistically high levels of inhibition.
*** Outcomes of the meeting
We are aiming for a set of punctual comparisons (not parameter sweeps)
between different network configurations. Initially, we are aiming to
show that we can reproduce some of the main results in the study with
a spiking network model with detailed synaptic connectivity. In other
words, for example, "show that the network with 4 dendrites is
significantly better than the network with 10 dendrites under
appropriate conditions". Because of the high computational costs
involved, this is more akin to what one would do experimentally than
to a numerical extension of the analytical results.
- network size: 500 GrCs
- use fixed tonic inhibition, set to an appropriate level (possibly
  the "natural" level of ~400pS).
- use Jason's average GrC model
- use my mf->GrC model built from Jason's data
- 1000 patterns
- 50 repetitions per pattern
- simulation time as short as possible compatibly with the duration of
  the integration window (hypothesised to be ~30ms in the paper,
  probably somewhere between that and 120ms, which is the longest time
  constant present in the synaptic mechanisms)
- number of dendrites d=4 and d=10
- use spatially correlated patterns. This should help in making the
  system more susceptible to variations in its parameters by making it
  harder to discriminate the patterns. Actually one could do both:
  correlated and uncorrelated patterns, and show the difference.
- use a pure rate code (rate in, rate out).
If there is time, we could add the following features:
- modulate sinusoidally input rate in time
- use the MUVR metric to cluster output 
**** DONE First assessment of the time involved in running simulations and analyses
     CLOSED: [2013-11-22 Fri 10:24]
***** DONE Back-of-the-envelope estimation of computational cost of simulations
      CLOSED: [2013-11-19 Wed 01:10]
This is with respect to my old runs back in spring 2012, which if I
remember correctly took about a week on matlem with 180 processors
(even though I might be overestimating this time if the batches of
jobs I was running were larger. Anyway, the values listed here were
for a simulation run size that I considered "typical", small enough to
be rerun if something went wrong).
|-------------------------+--------+--------+--------|
| dimension               | from   | to     | factor |
|-------------------------+--------+--------+--------|
| network size (GrCs)     | 60     | 500    | 8.3    |
| patterns                | 20     | 1000   | 50     |
| repetitions per pattern | 200    | 50     | 1/4    |
| simulation length       | 300ms  | 50ms   | 1/6    |
| parameter space points  | > 80   | < 8    | < 1/10 |
| 1/processors used       | ~1/180 | ~1/220 | 0.8    |
|-------------------------+--------+--------+--------|
| total                   |        |        | < 1.4  |
This of course assumes everything scales linearly and doesn't account
for overheads associated to running a larger number of smaller
simulations and having more complex synaptic models, but it is
encouraging, at least from the point of view of running the
simulations.
***** DONE Test how many cores I can actually use on the cluster
      CLOSED: [2013-11-20 Wed 10:03]
      - CLOSING NOTE [2013-11-20 Wed 10:03] \\
        It seems I can get about *238 cores*.
***** DONE see if there's any leftover issue with pymuvr
      CLOSED: [2013-11-22 Fri 10:17]
      - CLOSING NOTE [2013-11-22 Fri 10:17] \\
	Issue fixed in [[magit:~/phd/code/network/src/::commit@53ce7fb][commit #53ce7fb]]. This was just a matter of
        fixing a leftover bug in the integration with pymuvr.
***** DONE See if I can run the current version of the pipeline (including analysis) out-of-the-box for a problem of the order of magnitude we're interested in.
      CLOSED: [2013-11-22 Fri 10:20]
      - CLOSING NOTE [2013-11-22 Fri 10:20] \\
	The pipeline seems to work, but we'd be better off using a pure rate
	code for better performance.
***** Conclusion
My quick estimate above seems to hold for the simulation, but I think
that for the analysis we need to use a simpler decoder.
** Model-related and scientific choices that need to be made
*** TODO frequency response of the GrC model to synaptic stimulation
    I have to check if the 30ms integration time window hypothesis is
    justified, and adjust the length of the simulations accordingly.
    It looks like a good way of justifying this would be by looking at
    the autocorrelation function of the spike train produced by the
    cell model under Poisson stimulation at different rates. This
    actually shows that, for stimulation rates high enough to give a
    decent estimate of the ACF with a simulation 60s long, the
    autocorrelation seems to drop to its asymptotic value (the average
    firing rate) after about 50ms (see figures below, and note that
    the reported stimulation rate is for each of 4 MFs).
    
    Another interesing phenomenon can be seen in this experiment: as
    reported in Moreno-Bote and Parga 2006, there are two regimes
    depending on whether the average membrane depolarization is above
    or below threshold. Below threshold firing is driven by
    fluctuations and input spike bunching, while above threshold
    the cell tend to fire regularly and fluctuations only add noise to
    this process. This is reflected in a different structure of the
    ACF in the two cases.

    [[file:fig/acf_40Hz.png][file:fig/acf_40Hz.png]]
    [[file:fig/acf_40Hz_detail.png][file:fig/acf_40Hz_detail.png]]
    [[file:fig/acf_100Hz.png][file:fig/acf_100Hz.png]]
    [[file:fig/acf_100Hz_detail.png]]
    [[file:fig/acf_160Hz_detail.png]]

**** TODO Have a look at how Guy explains the 30ms assumption in the paper.
**** DONE Look at lower stimulation frequencies and longer simulations, to make sure these observations do not depend on the rates chosen.
     CLOSED: [2013-11-28 Thu 18:45]
     - CLOSING NOTE [2013-11-28 Thu 18:45] \\
       It's fine. Even with rates close to the cell threshold the
       autocorrelation falls to the 'uncorrelated baseline' in less than
       about 40ms.
*** output decoder: pure rate code vs spike metric
    The tradeoff between performance and added usefulness seems in
    favour of using a pure rate code, at least in the first stage as
    we use a rate-coded input.
*** DONE inhibition: decide on the (fixed) level of tonic GABA conductance
    CLOSED: [2013-11-27 Wed 12:26]
    - CLOSING NOTE [2013-11-27 Wed 12:26] \\
      The experimental value of 0.438nS seems the best option.
*** DONE mf -> grc synapses: decide if total conductance should scale inversely with the number of dendrites.
    CLOSED: [2013-11-28 Thu 18:44]
    - CLOSING NOTE [2013-11-28 Thu 18:44] \\
      No, it shouldn't change, otherwise altering the number of dendrites in
      the spiking model would have a very different meaning than in th
      binary model.
*** DONE Make a table of all the parameters in the model, noting where each of them comes from.
    CLOSED: [2013-11-28 Thu 16:21]
    - CLOSING NOTE [2013-11-28 Thu 16:21] \\
      This is ready to be sent to Angus.
    |----------------+----------------------------------+-----------+-----------------------|
    | model          | parameter                        | value     | source                |
    |----------------+----------------------------------+-----------+-----------------------|
    | Grc IaF        | leak reversal potential          | -79.9 mV  | Schwartz2012          |
    |                | leak conductance                 | 1.06 nS   |                       |
    |                | membrane capacitance             | 3.22 pF   |                       |
    |                | spike threshold                  | -40 mV    |                       |
    |                | reset potential                  | -63 mV    |                       |
    |                | refractory time                  | 2 ms      |                       |
    |----------------+----------------------------------+-----------+-----------------------|
    | GABA (tonic)   | reversal potential               | -79.1 mV  | Seja2012              |
    |                | conductance                      | 0.438 nS  | Rothman2009           |
    |----------------+----------------------------------+-----------+-----------------------|
    | AMPA direct    | reversal potential               | 0 mV      | Rothman2009 (new fit) |
    |                | amplitude 1                      | 3.724 nS  |                       |
    |                | amplitude 2                      | 0.3033 nS |                       |
    |                | rise time                        | 0.3274 ms |                       |
    |                | decay time 1                     | 0.3351 ms |                       |
    |                | decay time 2                     | 1.651 ms  |                       |
    |                | plasticity: initial release prob | 0.1249    |                       |
    |                | plasticity: depr. recovery time  | 131 ms    |                       |
    |----------------+----------------------------------+-----------+-----------------------|
    | AMPA spillover | reversal potential               | 0 mV      | Rothman2009 (new fit) |
    |                | amplitude 1                      | 0.2487 nS |                       |
    |                | amplitude 2                      | 0.2799 nS |                       |
    |                | amplitude 3                      | 0.1268 nS |                       |
    |                | rise time                        | 0.5548 ms |                       |
    |                | decay time 1                     | 0.4 ms    |                       |
    |                | decay time 2                     | 4.899 ms  |                       |
    |                | decay time 3                     | 43.1 ms   |                       |
    |                | plasticity: initial release prob | 0.2792    |                       |
    |                | plasticity: depr. recovery time  | 14.85 ms  |                       |
    |----------------+----------------------------------+-----------+-----------------------|
    | NMDA           | reversal potential               | 0 mV      | Rothman2009 (new fit) |
    |                | amplitude 1                      | 17 nS     | Schwartz2012          |
    |                | amplitude 2                      | 2.645 nS  | Schwartz2012          |
    |                | rise time                        | 0.8647 ms | Rothman2009 (new fit) |
    |                | decay time 1                     | 13.52 ms  | Rothman2009 (new fit) |
    |                | decay time 2                     | 121.9 ms  | Rothman2009 (new fit) |
    |                | plasticity: release probability  | 0.0322    | Rothman2009 (new fit) |
    |                | plasticity: depr. recovery time  | 236.1 ms  | Rothman2009 (new fit) |
    |                | plasticity: pot. recovery time   | 6.394 ms  | Rothman2009 (new fit) |
    |                | Mg2+ block: Z                    | 2         | Schwartz2012          |
    |                | Mg2+ block: T                    | 35 degC   | Schwartz2012          |
    |                | Mg2+ block: [Mg2+]               | 1 mM      | Schwartz2012          |
    |                | Mg2+ block: delta_bind           | 0.35      | Schwartz2012          |
    |                | Mg2+ block: delta_perm           | 0.53      | Schwartz2012          |
    |                | Mg2+ block: C1                   | 2.07 mM   | Schwartz2012          |
    |                | Mg2+ block: C2                   | 0.015 mM  | Schwartz2012          |
    |----------------+----------------------------------+-----------+-----------------------|
    | MF terminal    | refractory time                  | 1 ms      | Schwartz2012          |
**** DONE find an appropriate reference for the upper limit on MF firing rate
     CLOSED: [2013-11-29 Fri 19:05]
     - CLOSING NOTE [2013-11-29 Fri 19:05] \\
       Rancz2007 and references 8-11 therein should be enough.
*** TODO specify a form of input pattern correlation
When doing this, we should keep in mind the ~40um figure for
correlation of grc activity. Or rather +the inverse, ie+ the typical
distance between two glomeruli which share a common output, /which is
the same thing, right?/ (probably true for the mean values, but false,
in general, for the shape of the distribution describing the falloff of
correlation with distance).

We decided that we're going to do binary spatial tilings of the mossy
fiber terminals set. A good idea might be to choose the size of these
tilings in such a way that we can afford, say with 1024 patterns, to
explore all possible binary combinations. This might be tricky,
though, as we still need to vary p(MF) as an independent variable.
*** DONE finalise a choice for input stimulation rates
    CLOSED: [2014-01-06 Mon 14:34]
    - CLOSING NOTE [2014-01-06 Mon 14:34] \\
      I think this has been settled, with 80Hz for the active MFs and 10Hz
      for the inactive ones.
    In the paper, an input frequency range of about 10-50Hz was
    mentioned when justifying the 30ms integration window
    hypothesis. +My input patterns should live within that range as
    well.+ Actually, we're probably going to go for something around
    80Hz rather than 50Hz.
*** DONE finalise a choice for the sparsity measure
    CLOSED: [2014-01-06 Mon 14:33]
    - CLOSING NOTE [2014-01-06 Mon 14:33] \\
      I'v settled for the /activity sparseness/ measure, which together with
      deterministic input rates seems to give me a direct correspondence to
      p(MF) and p(GC).
    The best candidates at the moment are:
     - a properly normalised version of the Treves-Rolls sparsity
       measure, or something along those lines; for example the Hoyer
       sparsity measure as described in Hurley2009
     - what Willmore and Tolhurst (2001) call 'activity
       sparseness'.
    <2013-12-19 Thu> /reverse Hoyer/ is looking good at the moment,
    and that's what I'm using for most plots.

    <2013-12-25 Wed> actually, after removing "rate noise" (see
    [[magit:~/phd/code/network/src/::commit@87da7aa][commit #87da7aa]]) from the simulations, the /activity sparseness/
    has the very nice property of matching my control parameter for
    p(MF).
*** Plan B: is there an easy way of integrating synaptic noise in our analysis?
** Technical upgrades needed 
*** Simulation features
**** DONE Check that GrC and synaptic model actually correspond to the latest version from Rothman and Piasini 2013.
     CLOSED: [2013-11-22 Fri 13:10]
**** DONE Update the GABA reversal potential with the value reported in Seja2012.
     CLOSED: [2013-11-22 Fri 17:03]
     - CLOSING NOTE [2013-11-22 Fri 17:03] \\
       Updated the value in the GrC model file in the original OSB repo and
       in the if_gl neuroConstruct project.
**** DONE Change the tonic inhibition from being a current to being a conductance.
     CLOSED: [2013-11-22 Fri 16:59]
     - CLOSING NOTE [2013-11-22 Fri 16:59] \\
       Tonic GABA has been added as a conductance. Its value is fixed,
       though, and at the moment the "bias" parameter still controls an
       inhibitory current.
**** DONE If the simulation time is short, should the variables in the synaptic plasticity mechanisms be initialised at their steady state value for the given stimulation frequency?
     CLOSED: [2013-11-27 Wed 12:17]
     - CLOSING NOTE [2013-11-27 Wed 12:17] \\
       No, we'd better keep simulation time a bit longer (say 200ms) and only
       analyse the final window of the simulation (50, 100 or 150ms).
**** DONE Make a simple f-f curve for the GrC model with 4 dendrites.
     CLOSED: [2013-11-29 Fri 14:21]
     - CLOSING NOTE [2013-11-29 Fri 14:21] \\
       #+CAPTION: Comparison of the f-f curve for my model in LEMS and Jason's model cell number 156. The model receives 4 independent Poisson MF stimuli, plus tonic inhibition. Cell number 156 has been chosen because it's the one whose parameters are closer to the average of the population (ie it's an approximation of 'the average model'). Generated using [[magit:~/phd/nC_projects/GranCellRothmanIf/::commit@4671576][commit #4671576]] in the granule cell model repo.
       #+NAME: fig:rate_IO
       [[file:fig/rate_IO.png]]
**** DONE Make a comparison between my f-f curve and what Jason had in Schwartz2012.
     CLOSED: [2013-12-02 Mon 14:57]
     - CLOSING NOTE [2013-12-02 Mon 14:57] \\
       Figure [[fig:rate_IO]] shows that my model tends to fire a bit more
       than Jason's. This was expected, and (given that the cell
       models are very similar in the two cases) mostly depends on the
       time-averaged NMDA component of my synaptic model being
       somewhat larger. This, in turn, depends on the fact that both
       me and Jason normalised the NMDA waveform at -60mV to have the
       same peak amplitude as the AMPA component, with the two NMDA
       models having a different base waveform shape. Specifically, my
       waveform is somewhat less peaked than Jason's, so this results
       in a larger time-averaged conductance in my mode. Anyway, as
       can be seen in the figure below, the value for the
       time-averaged NMDA to AMPA ratio remains within the biological
       range as published in Schwartz2012.
       #+NAME: fig:NMDA_to_AMPA_ratio
       #+ATTR_HTML: :width 100%
       [[file:fig/NMDA_to_AMPA_ratio.png][file:fig/NMDA_to_AMPA_ratio.png]]
**** DONE +Implement Guy's network connection algorithm+
     CLOSED: [2013-12-02 Mon 10:30]
     - CLOSING NOTE [2013-12-02 Mon 10:30] \\
       No need for this; Guy is going to send me a bunch of network
       realisations exported from his code, and I'll have to import them at
       network creation time.
     Message from Guy:
     #+BEGIN_QUOTE
     Unless you have already solved it I would suggest an
     implementation such as the following

     i) generate glomerular degree distribution (that of a binomial)

     ii) for each (randomly chosen) connection chose a granule cell to
     connect to subject to the constraints:

     connection length as close as possible to 'x' (we used 15 microns I believe)
     granule cell would not have more than 'd' connections
     the connection does not repeat with a granule cell to which the glom. is already connectioned
     the connection would not cause a single mossy fibre to be connection to the same granule cell
     [The last one is a bit of an artificial one that had to be introduced because for high number of connections, the resampling because an problem since it diminishes the number
     of independent inputs to the network, which create conceptual issues]
       
     The above also assumes that all cells and connections are
     confined to the same sphere (connections do not penetrate the
     sphere holding the GRCs).

     The problem I am having is that I also definately had to create a
     method to shuffle some connections about with the aim of
     minimising the mean squared deviation from the target dendrite
     lenght. I think it might be that the above created a weird
     dendrite length distribution (i.e. not central). But I am not
     sure and I cant look at my Mathematica notebooks right now.

     So try the above. You might find it works in which case no
     problem. If there are any problems however, let me know because I
     will probably remember it at that stage!
     #+END_QUOTE
**** DONE [1/1] Reduce maximum number of files used by a complete run through of the workflow.
     CLOSED: [2013-12-05 Thu 11:13]
     - CLOSING NOTE [2013-12-05 Thu 11:13] \\
       Improved garbage collection in [[magit:~/phd/code/network/src/::commit@78beb48][commit #78beb48]]. Let's see if
       this is enough.
     - [X] Make sure nC garbage gets cleaned up before a simulation
       job exits.
**** DONE Use ~bias~ parameter to control tonic GABA.
     CLOSED: [2014-01-12 Sun 22:21]
     - CLOSING NOTE [2014-01-12 Sun 22:21] \\
       Done in [[magit:~/phd/code/network/src/::commit@0944195][commit #0944195]]. The name of the parameter is not ~bias~,
       though, but ~extra_tonic_inhibition~.
     The parameter has been renamed to ~extra_tonic_inhibition~ in
     [[magit:~/phd/code/network/src/::commit@6468623][commit #6468623]]. It still doesn't have any effect on the
     simulations, though.
**** DONE Implement DTA
     CLOSED: [2014-01-14 Tue 23:58]
     - CLOSING NOTE [2014-01-14 Tue 23:58] \\
       Done in [[magit:~/phd/code/network/src/::commit@64cf9fe][commit #64cf9fe]].
**** DONE Implement simple sinusoidally varying input
     CLOSED: [2014-01-14 Tue 23:59]
     - CLOSING NOTE [2014-01-14 Tue 23:59] \\
       Done in [[magit:~/phd/code/network/src/::commit@6cbb7f6][commit #6cbb7f6]]. For the time being, the modulation
       frequency is a parameter (so that it can be set to 0 or 8 in
       the simplest comparison), but the amplitude of the modulation
       is hardcoded at 50%.
**** DONE Fix csv export of glomerular positions in Guy's mathematica notebook.
     CLOSED: [2013-12-08 Sun 19:03]
     - CLOSING NOTE [2013-12-08 Sun 19:03] \\
       Done. This required changing his Mathematica library to add a function
       to compute an adjacency matrix for the network composed by glomeruli
       and granules (as opposed to the network composed by mossy fibres and
       granules), and updating the network generation notebook accordingly.
     It seems like glomerular positions are expressed as a list of
     lists of 3d points, but the length of the lists of points is not
     constant. Also, the graphml file seems to contain a glomerular
     node for each list, and not for each point. What are the extra 3d
     points for? what's the correspondence between nodes in the graph
     and spatial coordinates? Maybe this has something to do with
     having the glomeruli grouped by the MFs to which they belong, and
     then keeping only one glomerulus per MF at some point downstream
     in the code during network building.
**** DONE Implement importer for text-based network descriptions.
     CLOSED: [2013-12-08 Sun 19:05]
     - CLOSING NOTE [2013-12-08 Sun 19:05] \\
       Done in [[magit:~/phd/code/network/src/::commit@63aacda][commit #63aacda]], and added a script in [[magit:~/phd/code/network/src/::commit@ebe4d78][commit #ebe4d78]]
       which takes care of setting up the data directories and to copy
       the graphml files in place for a predefined set of network
       structure configurations. Shown below in figure
       [[fig:tissue_model_3d_view]] is a 3D view (made with the
       [[file:scripts/visualise_network.py::filename%20%3D%20'/home/ucbtepi/code/network/data/network_structures/gd4/GCLconnectivity_full.graphml'][mayavi-based network visualisation script]]) and in figure
       [[fig:tissue_model_2d_view]] a (x, y) projection (made with [[https://gephi.org][Gephi]])
       of an instantiation of the network with 4 dendrites. Figure
       [[fig:tissue_model_projected]] shows a map of the number of
       degrees of separation between one granule node (in red) and the
       other nodes. A correlation between euclidean and separation
       distance is evident.
       #+CAPTION: three-dimensional view of a network exported from the Mathematica toolbox.
       #+NAME: fig:tissue_model_3d_view
       #+ATTR_HTML: :width 60%
       [[file:fig/tissue_model_3d_gd4.png]]

       #+CAPTION: (x, y) projection of an exported network. Red: glomeruli; light blue: granule cells.
       #+NAME: fig:tissue_model_2d_view
       #+ATTR_HTML: :width 60%
       [[file:fig/network_bipartite_gd4.png]]

       #+CAPTION: projection of the network onto the set of granule cells, color-coded by the shortest path distance from a specific node.
       #+NAME: fig:tissue_model_projected
       #+ATTR_HTML: :width 60%
       [[file:fig/network_projected_gd4.png]]

       #+CAPTION: same as figure [[fig:tissue_model_projected]] but after randomizing the edges of the graph.
       #+NAME: fig:random_bipartite_projected
       #+ATTR_HTML: :width 60%
       [[file:fig/network_projected_randomised_gd4.png]]

**** TODO Implement input correlations.
**** DONE Substitute Jython 2.5 with 2.7b1 in nC, test and make a pull request
     CLOSED: [2013-12-09 Mon 23:03]
     - CLOSING NOTE [2013-12-09 Mon 23:03] \\
       [[http://bugs.jython.org/issue2070][An issue]] with Jython 2.7b1 prevents recent versions of xlrd to be
       imported. xlrd is a package that was bundled with nC/Jython upon
       request by the OpenWorm people. It's possible to bundle an earlier
       version, though, which would still be newer than the one we currently
       offer. I've made a pull request and there is [[https://github.com/NeuralEnsemble/neuroConstruct/issues/33][a discussion]] going on on
       github at the moment. We'll see what comes out of it.
**** DONE Fix out of memory error for qstat poller
     CLOSED: [2013-12-19 Thu 13:44]
     - CLOSING NOTE [2013-12-19 Thu 13:44] \\
       The qsat poller has been entirely done away with since [[magit:~/phd/code/network/src/::commit@c93b5d3][commit #c93b5d3]].
     #+BEGIN_SRC sh
     SIM: 172 running, 68 waiting, 0 other jobs, 473 in the pre-queue
     Traceback (most recent call last):
       File "master_script.py", line 85, in <module>
	 batch_manager.update_status()
       File "/home/ucbtepi/code/network/src/utils/queue.py", line 126, in update_status
	 self.simulation.update_jobs_and_check_for_CME()
       File "/home/ucbtepi/code/network/src/utils/queue.py", line 56, in update_jobs_and_check_for_CME
	 self.update_job_sets()
       File "/home/ucbtepi/code/network/src/utils/queue.py", line 53, in update_job_sets
	 self.waiting_jobs = self._managed_jids_from_qstat('-s', 'p')
       File "/home/ucbtepi/code/network/src/utils/queue.py", line 21, in _managed_jids_from_qstat
	 stat_lines = Popen(itertools.chain(['qstat'], qstat_argument_list), stdout=PIPE, stderr=PIPE).communicate()[0].split('\n')
       File "/share/apps/python/lib/python2.7/subprocess.py", line 672, in __init__
	 errread, errwrite)
       File "/share/apps/python/lib/python2.7/subprocess.py", line 1112, in _execute_child
	 self.pid = os.fork()
     OSError: [Errno 12] Cannot allocate memory
     #+END_SRC
     This is probably caused by the fact that (from [[http://stackoverflow.com/a/13329386][a question on
     stack overflow]])
     #+BEGIN_QUOTE
     subprocess.Popen uses fork/clone under the hood, meaning that every
     time you call it you're requesting once more as much memory as Python
     is already eating up.
     #+END_QUOTE
     The solution is to reimplement qstat poller as a shell script
     spawned at the start of ~master_script~. This poller should keep
     probing the queue status on a loop (qstat;wait;etc) and output to
     stdout. ~master_script~ should just 'poll the poller' by using
     its ~stdout~ attribute (see [[http://docs.python.org/2/library/subprocess.html#subprocess.Popen.stdout][the suprocess docs]]).
**** DONE Enable reuse of larger dataset whenever they are available from previous simulation runs/parameter sweeps.
     CLOSED: [2013-12-25 Wed 00:32]
     - CLOSING NOTE [2013-12-25 Wed 00:32] \\
       Done in [[magit:~/phd/code/network/src/::commit@cff5a71][commit #cff5a71]].
     The use case here is the following: I have already simulated a
     given parameter space point with 1024 patterns, 100 repetitions
     and simulation length of 200ms, and now I want to simulate and
     analyse another parameter space point which is the same but with
     128 patterns, 50 repetitions and simulation length of 150ms. It's
     a waste of time and disk quota to rerun new simulations and
     create a new spike archive! what should happen is that the spike
     trains that are needed for the analysis get loaded from the
     existing archive.
**** DONE +Don't simulate a given pattern if its corresponding tar archive is already on disk+
     CLOSED: [2014-01-12 Sun 18:17]
     - CLOSING NOTE [2014-01-12 Sun 18:16] \\
       This additional robustness measure shouldn't be necessary after
       implementing BLCR checkpointing in [[magit:~/phd/code/network/src/::commit@e3fd535][commit #e3fd535]].
**** DONE +Make sure all tar single-pattern archives are present before starting compression step+
     CLOSED: [2014-01-12 Sun 18:17]
     - CLOSING NOTE [2014-01-12 Sun 18:17] \\
       After implementing BLCR checkpointing in [[magit:~/phd/code/network/src/::commit@e3fd535][commit #e3fd535]], simulation
       jobs should complete successfully even in case of poor harware
       performance on the first node they're submitted to. This means we can
       rely on the .tar archives being there. If they're not, it's ok to
       keep the current behaviour: remove all the tar files up to the missing
       one, and then raise an exception.
     This, togheter with the previous item, should allow easy re-running
     of specific patterns if something goes wrong during simulations and
     not all patterns for a given data point reach completion.
*** Analysis features
**** DONE Implement pure rate-code decoder with k-means clustering.
     CLOSED: [2013-11-22 Fri 10:54]
     - CLOSING NOTE [2013-11-22 Fri 10:54] \\
       Done in [[magit:~/phd/code/network/src/::commit@1e7745c][commit #1e7745c]].
**** DONE check if analysis workflow can handle a problem of the scale we're interested in
     CLOSED: [2013-11-25 Mon 16:00]
     - CLOSING NOTE [2013-11-25 Mon 16:00] \\
       The whole workflow can run overnight for a single parameter space
       point, 1000 patterns, 50 repetitions, 150ms long simulations, and
       rate-coded k-means based decoding.
**** DONE [2/2] Implement sparsity measure.
     CLOSED: [2013-12-03 Tue 16:22]
     - CLOSING NOTE [2013-12-03 Tue 16:22] \\
       Implemented both 'activity' and 'hoyer' sparseness measures in [[magit:~/phd/code/network/src/::commit@0577178][commit #0577178]].
     - [X] Implement measure candidates, so that I can play around
       with them while we decide on which ones to use.
     - [X] Add sparseness estimation to analysis workflow.
**** DONE Take info on number of cells from archive metadata and not from size of spike tables.
     CLOSED: [2013-12-08 Sun 19:08]
     - CLOSING NOTE [2013-12-08 Sun 19:08] \\
       Done in [[magit:~/phd/code/network/src/::commit@d239378][commit #d239378]].
**** DONE Add dimension to analysis parameter space to represent integration window length.
     CLOSED: [2013-12-25 Wed 00:18]
     - CLOSING NOTE [2013-12-25 Wed 00:18] \\
       Done in [[magit:~/phd/code/network/src/::commit@6468623][commit #6468623]]. The size of both simulation and analysis
       time windows are now parameters.
**** DONE Remove saving of all rates to results archives before running with big network
     CLOSED: [2013-12-08 Sun 19:13]
     - CLOSING NOTE [2013-12-08 Sun 19:13] \\
       Done in
       [[magit:~/phd/code/network/src/::commit@8564526][commit #8564526]].
**** DONE Reintroduce separation between training and testing data sets
     CLOSED: [2013-12-25 Wed 00:19]
     - CLOSING NOTE [2013-12-25 Wed 00:19] \\
       Done in [[magit:~/phd/code/network/src/::commit@3373e73][commit #3373e73]].
**** TODO Test decoder performance for convergence speed (performance vs size of training set)
**** TODO Test decoder performance for rubustness against noise (performance vs size of testing set)
*** DONE Get the whole workflow to run on *Legion*.
    CLOSED: [2013-12-26 Thu 00:37]
    - CLOSING NOTE [2013-12-26 Thu 00:37] \\
      Done but not committed for the time being, as I'm not sure how
      to deal with jobscripts customised for different clusters from
      the versioning standpoint. The issue was indeed linked to the
      environment of the jobs sumbitted to the queue: I had to take
      out the ~-V~ sge option to make JSV happy, and I forgot to make
      up for it by changing the first line of the jobscripts from
      ~#!\bin\bash\~ to ~#!\bin\bash\ -l~. This meant that my
      ~.bashrc~ was not read by the jobs running on the compute nodes,
      so that no module was being loaded (as in ~module load ...~).
    <2013-12-25 Wed> Most things seem to work after properly
    customising the jobscripts to the environment. The main problem at
    the moment must be linked to some weird environment setting, and
    NEURON having issues compiling mod files. This only happens for
    jobs I submit into the queue, as everything seems to run smoothly
    on the /user test nodes/.
** Meeting with Angus and Guy, pre-christmas break <2013-12-13 Fri> 
*** Points for discussion
1. *integration time window*: 
   - I should probably implement it as a free parameter that only
     requests for simulations to be rerun if the new length is longer
     than what's available
   - Are we still going to parameter-sweep in this dimension? it's
     probably worth at least some exploratory analyses, especially
     because after the data management part is implemented it's a
     relatively cheap thing to do (doesn't require additional
     simulations)
2. *inhibition*:
   - implement as a free parameter (using ~bias~?)
   - Are we still going to do the analysis where we see what happens
     to MI in the many-dendrites case if we try to bring
     sparsification levels down to what the few-dendrites network do?
     I think this is a good idea, and since I don't expect
     sparsification to depend on the number patterns we can use
     simulations with few patterns to map sparsification, and then use
     this mappings to select the parameter space points where we want
     to calculate MI for the larger network.
3. Actually, this is in general a good idea: we can calculate
   sparsification for a much wider range of parameters than MI, so we
   should make use of this.
4. *spatial structure*: discrete parameter to switch between random and
   spatial graph I already have randomised replicas of each network
   realisation, so it's just a matter of selecting a different graphml
   file.
5. *input correlation*:
   - how I am going to map the idea of P(MF) to the continuous case
     with gaussian blobs of activation in space? I could do this by
     manipulating the variance, but I have to keep in mind that a
     single gaussian spatial profile will give different average input
     rates depending on where it's centered with respect to the
     border of the tissue sphere. An alternative for working around
     this issue would be to normalise the rate profiles over the set
     of nodes, so that we are sure we always get the same input
     rate. But then again this would effectively alter the sparsity of
     the input pattern.. we need to think if there's a way of 
   - once I've implemented it, apart from the parameters I have
     already to specify mean and fluctuations for rates, I will also
     need a discrete parameter to switch between correlated and
     uncorrelated input.
   - I need to be careful though when confronting network activity for
     correlated and uncorrelated input, as - I guess - I want to make
     sure that the average input rate is the same. so, for example,
     gaussian blobs with peak at 50Hz superimposed over a noisy
     backgorund of 10Hz shouldn't be translated directly to binary
     patterns with 10/50Hz activation; alternativly, the
     transformation should happen very carefully with respect to the
     resulting P(MF).
6. intra-pattern vs inter-pattern correlations
7. should we tune the input correlations to the spatial correlation
   scale in the grcs? This should boost information transmission, but
   we don't know how uniformly across parameter space.
8. in the paper, what will we test with the biologically constrained
   network? We need a clear hypothesis, so that we can test it and
   argue about how highly constrained is the spiking network and why
   it's a good testbed for our hypothesis.
9. should we use some more graph theory to expand on the differences
   between random and anatomical models? See figures
   [[fig:tissue_model_projected]] and [[fig:random_bipartite_projected]].
10. input frequency range? shall we go from 10/50Hz to 10/100Hz? This
    would probably shift the activity for high ~d~ well into
    saturation, thus helping us to make our point.
*** Outcomes of the meeting
**** DONE integration time window
     CLOSED: [2013-12-25 Wed 00:20]
     - CLOSING NOTE [2013-12-25 Wed 00:20] \\
       Done in [[magit:~/phd/code/network/src/::commit@6468623][commit #6468623]].
See above. Big simulations with a thousand of patterns should be run
for a longer time, so that in case we have room for investigating the
effect of shortening the time window. It needs to be
promoted to being a parameter anyway.
**** DONE Implement a discrete parameter to switch between random and anatomical graphs.
     CLOSED: [2013-12-25 Wed 00:22]
     - CLOSING NOTE [2013-12-25 Wed 00:22] \\
       Done in [[magit:~/phd/code/network/src/::commit@6468623][commit #6468623]].
**** DONE Implement a discrete parameter to switch between correlated and uncorrelated inputs.
     CLOSED: [2013-12-25 Wed 00:22]
     - CLOSING NOTE [2013-12-25 Wed 00:22] \\
       Done in [[magit:~/phd/code/network/src/::commit@6468623][commit #6468623]].
**** TODO I have to improve my plots, and in particular make some output sparsity vs input sparsity plots, which would make it easier to visualise ranges over which the network sparsifies.
**** DONE Some more quantitative analysis on the difference in structure between the random and spatial graphs.
     CLOSED: [2014-01-06 Mon 14:02]
     - CLOSING NOTE [2014-01-06 Mon 14:02] \\
       Done in [[magit:~/phd/code/network/src/::commit@5c4cb66][commit #5c4cb66]]. A useful reference for statistics on
       bipartite graphs is Latapy et al 2008, /Basic notions for the analysis
       of large two-mode networks/. Various measures of clustering and
       "bipartite redundancy" exist, but in the end I settled on the simplest
       one (called Robins-Alexander clustering in networkx) which is just the
       number of 4-cycles in the graph normalised by the maximum number that
       would be allowed by the size of the two sets of nodes. Figure
       [[fig:graph_clustering]] shows how the spatial structure of the tissue
       model makes it more clustered than the random graph and how the
       clustering measure increases with the number of dendrites for both
       models.
     #+NAME: fig:graph_clustering
     #+CAPTION: Simple bipartite clustering measure for the tissue model and the random bipartite network.
     [[file:fig/graph_clustering.png][file:fig/graph_clustering.png]]

**** DONE Quantitative, numerical measure of sparsification
     CLOSED: [2014-01-06 Mon 14:28]
     - CLOSING NOTE [2014-01-06 Mon 14:28] \\
       This has become less of a problem now that the input rates are
       deterministic and that I'm using /activity sparseness/ to get a fairly
       close analogue to p(MF) and p(GC). If the need arises, we can still
       use the /amplification/ idea.
At the moment, I am using /amplification/ (P(GC)/P(MF)) and Guy is
using /sparsification/ (1-P(GC)/P(MF)). I like /amplification/ because
it's not a negative (ie saying 'network A has lower sparsification
than network B' sounds more ambiguous than saying 'lower
amplification'), but then again 'amplification' has other ambiguities,
as in the spiking network it could be taken to represent some measure
of /gain/ in rates between the inputs and the outputs. In conclusion,
I think either measure would work if we find an appropriate name for
it, and I don't think we need a different mathematical
characterisation of this quantity.
**** Sparsification as an average property of the network transfer function
It's a good idea to make use of the fact that sparsification is an
average property of the network transfer function (maybe make a test
for this?). This implies that we can map out sparsification for a wide
range of points in parameter space, by simulating just a small number
of patterns (we don't even require repetitions, in theory), using this
information to guide our intuition as to where to calculate MI.
**** Input correlations
First of all, our main figure is still going to be about uncorrelated
patterns, as this is the case which is most exhaustingly investigated
in the rest of the paper. Unless we find something extremely
interesting, correlated inputs are going to end up in the
supplementary information anyway.

An example of a particularly interesting thing that might happen is if
we got a sudden boost to information transmission in the anatomical
model when the spatial scale of the input correlations matches the
typical scale of network connectivity.

There are several issues with using gaussian blobs, mainly linked to
the fact that they are unlike our uncorrelated binary patterns /in too
many ways/. One way of introducing spatial correlations without
unnecessarily complicating things is to use *binary tilings* of the
set of glomeruli. Something like an n-nearest-neighbor tiling, which
(as opposed to eg a Voronoi tiling) would allow for exact control over
P(MF).
**** Input rates
We should raise the upper bound of our range of input rates, as this
will make the effect we're looking for more evident, but of course
while staying within what's known from the biology.
**** Role of inhibition
We think that we will be testing the effect of varying level of
inhibition in two situations:
- what would happen if there was a homeostatic mechanism which scaled
  tonic inhibition to match the increase in average excitatory
  conductance we get when increasing the number of dendrites?
- what happens to information transmission if we increase inhibition
  to force networks with many dendrites to sparsify as well as
  networks with fewer dendrites? (sort of like what Guy did in his new
  examples for figure 5)
**** Type of analysis we're going for
If I can, I should try and estimate the efficient frontier by
calculating the sparse encodable range for a sufficiently large number
of network configurations. If this proves to be too computationally
expensive, I should try and make some 'point' measurements of
performance in parameter space, as one would do in the case of an
actual experimental test, trying to analyse a few significant
points. Another idea to keep in mind is to try and be smart about how
I try to determine the sparse encodable range, perhaps by integrating
some sort of nonlinear optimisation technique (but we might be too
short with time for designing that), or some simpler system for not
wasting time exploring the big information plateau in the interior of
the encodable range for a given network configuration.
**** Figure 7 in the paper:
The point we're trying to make is that this is a very stringent test
of our hypothesis, as we're building the spiking network from the
bottom up with respect to the available experimental data on cell and
synaptic physiology and tissue anatomy.

To do this we need:
***** DONE 3D representation of the network with input and output raster plots
      CLOSED: [2013-12-16 Mon 12:31]
      - CLOSING NOTE [2013-12-16 Mon 12:31] \\
	3d representation implemented with Mayavi in [[magit:~/phd/code/network/src/::commit@6f6da5b][commit #6f6da5b]], and
	raster plot implemented in [[magit:~/phd/code/network/src/::commit@a42cb02][commit #a42cb02]].
***** TODO graph-theoretical projection onto the set of GrCs showing the range of input sharing
***** TODO heatmap plot or efficient frontier plot where we actually make our point.
** Meeting with Angus <2013-12-18 Wed>
We decided it's probably a good idea to take out the noise on the
input rates (ie the fact that at the moment the /on/ and /off/ rates
are not fixed values, but rather distributions over rates) and to
separate out again the simulated patterns into a /training/ and a
/testing/ set for the clustering decoder. This way we can claim that
the training phase represents some past activity, when the downstream
network had the chance of tuning in an unsupervised manner to the
typical classes of output of the granular layer.
** Meeting with Angus and Guy <2014-01-08 Wed>
   In the paper we already have 6 figures, on a budget of 8. We can
   'spend' the two remaining figures to explain my work on the spiking
   network. The idea underlying this part of the paper is that we can
   get similar results to what has been shown until now if we start
   from a bottom-up biophysically realistic model of the network.
*** TODO [4/5] figure 7: presentation of the model.
    The message we want to get across is that the model is tightly
    constrained to experimental data, and it is way more 'biological'
    than the binary network. This figure (together with the results,
    of course) should be key in addressing the concerns expressed by
    reviewer 2 and 3 that the binary model is too far removed from the
    biology and not different enough from what had been used in
    previous works on cerebellar function, and that the typical Neuron
    readership could find it difficult to relate to the methods we
    use. The figure should contain:
    - [X] f/f curve for cell model with (optionally) experimental data
      for comparison
    - [X] example voltage trace, possibly with a cartoon showing the
      granule cell with its 4 inputs and the conductance trains being
      injected. Tonic GABA, as well, should be shown for comparison.
    - [X] example of synaptic dynamics with fit to experimental data
    - [X] +3D visualisation of network structure with+ input and output
      raster plots
    - [ ] cartoon explanation of MI and data analysis (spike counting
      and clustering)
      - [ ] make sure resolution of plot with clustered barcodes
        matches the plot where the barcodes are shuffled.
*** TODO [0/3] figure 8: results. 
    This should contain:
    - [ ] 'raw data' in the form of a MI heatmap for 1024 patterns, to
      show qualitative match to figure 3A in the simplest possible
      case (with fixed inhibition). In the heatmap, we should indicate
      the region where amplification is less than 1. We should either
      spend some words to address the fact that 1024 is a very small
      number compared to 1 billion patterns, by claiming that
      increasing the number of patterns should only make the reduction
      of encodable range stronger, or show this result alongside a
      version of figure 3A opportunely scaled down to 1024 patterns.
    - [ ] sparsification heatmap.
    - [ ] global results in form of an 'efficient frontier' kind of
      plot, populated with data from at least 2-3 DTA
      settings. Because of our constraints in time and computational
      resources, this will have to be done with a smaller number of
      patterns than the previous figure (128 patterns should be
      feasible).
*** DONE Mockup of figures 7 and 8
    CLOSED: [2014-01-12 Sun 18:23]
    Angus need this as soon as possible to start writing.
*** TODO Proof-of-principle figure with sinusoidally varying input rate
    This should go in the supplementary material, and it will be used
    to show that our ideas still hold in case of sinusoidally varying
    input, at least for the simplest analysis possible. In particular,
    having an oscillating input will show that having the STP dynamics
    doing something a bit more interesting desn't compromise the big
    picture. This figure will also be key to (at least partially)
    address some of reviewer 1's comments on the role of cerebellar
    oscillations. The main idea is to have, instead of a fixed 80Hz
    rate for the 'on' mossy fibre terminals, an inhomogeneous Poisson
    process that will average at 80Hz but whose intensity will
    oscillate at some carrier frequency (50% modulation at 8Hz is what
    we're going for). Background activity from 'off' fibers
    will not be modulated.
    
    It would be nice to have a representation of what kind of input
    we're playing into the network in this case as well. something
    like a raster plot or a visualisation of the conductance trains.
*** TODO Detailed check/proofread of the Appendix to the supplementary material.
* Questions to be addressed with the model after we're sure it works
** Is the network with d=4 any better than the one with d=10 at information transmission or sparsification?
   My guess is that, unless our choice of input correlations make a
   massive difference, the d=10 network will be just as good as the
   d=4 network at transmitting information, mainly because we are
   operating with such a small number of patterns and so far away from
   channel capacity. What /could/ be different, though, is the
   sparsification: in that case, if the d=10 network sparsifies less,
   we can then ask: /what is the required amount of tonic inhibition
   to bring sparsification back to where it was with d=4? and what
   happens to information transmission if we set inhibition to that
   value?/ This would also offer a good way of formulating an
   experimentally testable hypothesis: if information transmission is
   still good even in this heavily inhibited case, then this is the
   homeostatic compensation we expect to arise in a mutant mouse with
   large d; otherwise, we expect to see some other mechanism coming
   into play, but still something that would tend towards allowing
   complete information transmission with a sparse code (for example,
   conformational changes in the granule cell's ion channels or a
   decrease in the amount of neurotransmitter released at the mossy
   fibre synapse). Or the mutant mouse's cerebellum might not function
   as well as the wild type's, of course.
*** Pushing the network to its limits <2013-12-04 Wed>
    We assume that the purpose of the network is to strike a balance
    between information transmission and sparsification. Some of the
    parameters in the simulations are relatively free (like input
    correlations and integration time window), and some can be changed
    but we have an idea of what they should be in a 'normal' network
    (like the amount of tonic inhibition). We want to compare, say,
    the network with 4 dendrites to the network with 20 dendrites.
**** Free parameters (input correlation, length of integration time window)
     We start by estimating, in parameter space, the edge of the
     region where the network with 4 dendrites transmits all of the
     information. In practice, this means finding the maximum amount
     of correlations in the inputs or the shortest integration window
     we can use before we start losing information. We then see how
     the network with 20 dendrites performs in these "extreme"
     conditions: our guess is that it will transmit less information
     for strongly correlated inputs and it will sparsify
     (proportionately) less for short integration time windows. My
     idea, though, by looking at the f-f curve in figure [[fig:rate_IO]]
     and considering that our input rates won't go above 50Hz, is that
     _at about 100ms we are already fairly close to the lower limit
     for the integration window in the network with 4 dendrites_, as
     already with 4 30Hz inputs we are below the 10Hz firing threshold
     for detection of a single spike in such a time interval.
**** Tonic inhibition
     Tonic inhibition is not really a free parameter of our model, and
     also has a very direct and obvious effect on sparsification. My
     proposal is that we treat it as an example of a possible
     homeostatic mechanism that could be used by the organism to
     balance the extra excitation the GCL would get if the number of
     dendrites was larger. We can then proceed as follows: we look at
     the network in 'normal' conditions and we note the MI and the
     sparsification, then we increase the number of dendrites from 4
     to 20 and we estimate again these two measures. Presumably, MI
     would be the same (if it's not, all the better) but there will be
     less sparsification. We could then increase inhibition to a level
     that will bring sparsification back to what we had with 4
     dendrites, and observe if there is any decrease in transmitted
     information. If that's the case, we can conclude that it's not
     possible to achieve the same optimal tradeoff between information
     and sparsity in a network with many dendrites without resorting
     to some more complicated homeostatic mechanism.

*** 1/r benchmark as mentioned in the discussion?

*** DONE Once I have a sparsity measure, make some exploratory simulations (even with the random graph model for the network) to have a rough idea of the behaviour of the MI and the sparsity for the cases with, say, 4 and 20 dendrites.
    CLOSED: [2014-01-06 Mon 14:32]
    <2013-12-03 Tue> Preliminary simulations indicate that, at least
    for the random bipartite graph and uncorrelated inputs, and for an
    initial fraction of active mossy fibers of 0.5, the results on
    sparsification are what we expect:
    | dendrites | in sp (activity) | out sp (activity) | in sp (hoyer) | out sp (hoyer) |
    |-----------+------------------+-------------------+---------------+----------------|
    |         3 |              0.5 |              0.85 |          0.27 |           0.61 |
    |         9 |              0.5 |              0.14 |          0.26 |           0.09 |
    We can also note that, at least in this case, the /activity
    sparseness/ measure seems to reflect better our intuition (I
    wonder if the Treves-Rolls measure would perform better than the
    Hoyer measure, here, the main difference between the two being a
    square root operation). I'm running now a parameter sweep over
    ~n_grc_dend~ and ~active_mf_fraction~ with a relatively small
    number of simulated patterns (100) to understand if this is a
    fluke or not, and more generally to compare the behaviour of the
    two sparseness measures across a wider range of conditions.
    
    <2013-12-05 Thu> The results from the parameter sweep look
    encouraging in terms of behaviour of the 'activity' sparseness. It
    looks like a (random) network with 4 dendrites sparsifies all
    the time, one with 6 dendrites keeps the sparseness roughly the
    same between input and output, and networks with 8 or 10 dendrites
    always amplify.

    <2013-12-11 Wed> From the point of view of information
    transmission, it looks like *the tissue model behaves very
    differently than the random bipartite graph*, especially at high
    p(MF).

    <2013-12-29 Sun> A wide parameter sweep for 128 patterns with the
    tissue model and no rate noise (figures [[fig:MI_128]] and
    [[fig:amp_128]]) shows no big surprises: networks with 4 (or
    possibly 5) dendrites are the only ones which transmit most of the
    information over a wide range of p(MF) while at the same time
    sparsifying. In particular, it's useful to look at a 'masked' mi
    heatmap (figure [[fig:MI_masked_128]], where we show the amount of
    transmitted information over the region of parameter space where
    we have sparsification.

    #+CAPTION: Mutual information for 128 patterns, 50 trials (30 for training), 80/10Hz.
    #+NAME: fig:MI_128
    [[file:fig/mi_128.png][file:fig/mi_128.png]]

    #+CAPTION: Amplification for 128 patterns, 50 trials, 80/10Hz.
    #+NAME: fig:amp_128
    [[file:fig/amp_128.png][file:fig/amp_128.png]]

    #+CAPTION: 'Masked' mutual information over total input entropy for 128 patterns, 50 trials (30 for training), 80/10Hz. Only regions in parameter space where the network sparsifies are shown.
    #+NAME: fig:MI_masked_128
    [[file:fig/mi_masked_128.png][file:fig/mi_masked_128.png]]
    
* Controls for supplementary information
** DONE Fix crash that happens when trying to analyse more than about 1024 (patterns) * 70 (trials) observations
   CLOSED: [2014-01-12 Sun 18:26]
   - CLOSING NOTE [2014-01-12 Sun 18:26] \\
     I believe this was due to excessive memory usage when counting
     spikes. [[magit:~/phd/code/network/src/::commit@68226f5][commit #68226f5]] should have fixed that.
Is this some sort of memory issue?
#+BEGIN_SRC
Traceback (most recent call last):
  File "analyse.py", line 9, in <module>
    point.run_analysis()
  File "/home/ucbtepi/code/network/src/utils/parameters.py", line 136, in run_analysis
    i_level_array = self.spikes_arch.get_spike_counts(cell_type='mf')
  File "/home/ucbtepi/code/network/src/utils/archival.py", line 34, in get_spike_counts
    spike_counts = np.array([[np.sum(c > start_time) for c in np.array(o).transpose()] for o in observation_handles])
  File "/home/ucbtepi/virtualenv/lib/python2.7/site-packages/h5py/_hl/dataset.py", line 609, in __array__
    self.read_direct(arr)
  File "/home/ucbtepi/virtualenv/lib/python2.7/site-packages/h5py/_hl/dataset.py", line 574, in read_direct
    self.id.read(mspace, fspace, dest)
  File "h5d.pyx", line 179, in h5py.h5d.DatasetID.read (h5py/h5d.c:2655)
  File "_proxy.pyx", line 118, in h5py._proxy.dset_rw (h5py/_proxy.c:1487)
  File "_proxy.pyx", line 84, in h5py._proxy.H5PY_H5Dread (h5py/_proxy.c:1237)
IOError: can't read data (Dataset: Read failed)
Command exited with non-zero status 1
647.71user 4.11system 33:52.91elapsed 32%CPU (0avgtext+0avgdata 12994880maxresident)k
0inputs+0outputs (1major+831036minor)pagefaults 0swaps
#+END_SRC
** TODO Bias: MI vs size of testing set or number of clusters
** TODO Decoder performance: MI vs size of training set
** TODO Sparse encodable range vs integration window length
* Other ideas
*** Testing for 'randomness' in the mf->grc connectivity
If it were possible to measure the correlation range of the activity
of grcs following signals from the mfs given with a known spatial
correlation (see work by Mapelli?), then given the measurements of
the length of the grc dendrites we should be able to test if the
mf->grc connections are 'random' (ie after some opportunely
constrained maxent distribution) by predicting the correlation range
and seeing if it matches with the experiments.
