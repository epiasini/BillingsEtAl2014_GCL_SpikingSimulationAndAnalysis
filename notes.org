* Is it possible to use my simulations to support the main claim made in Guy's paper?
** Meeting with Angus and Guy <2013-11-19 Tue>
*** Points for discussion
1. what cell model? should I use my new cell model population? in this
   case, what about the single-cell-level disorder? given that's
   impossible to treat this by annealing (ie averaging over it), at
   which point should I quench it (ie fix network configuration)? An
   alternative is to use the average model from my population.
2. I also need to remember to update the value of the GABA reversal
   potential with the figure from Seja2012.
3. Speaking of quenched disorder, what did Guy do for the
   connection-driven disorder?
4. I guess we're going to use my new model for the mf->grc
   synapse.. this means using jLEMS (ie suicide) unless we get the
   network simulation export function to run. Or do I? probably not,
   if the network is still described with neuroConstruct.
5. What exactly do we mean with "testing with rate code"? if it's only
   rate in-rate out, we don't need to use Conor's metric (or more
   precisely, we don't need a Van Rossum-type metric), as we can just
   use spike counts.
6. How to measure sparsity in the rate-coded case? An interesting idea
   would be to use the Gini index, but this is a measure of inequality
   and would give sparsity=1 in the case of a silent network. If by
   sparsity we really mean "global activation level", I can just use
   the average firing rate of the network, but in that case I would
   still have to normalize it somehow to compare it to the firing rate
   of the inputs (to impose the sparsifying condition p(GC)<p(MF)).
7. I guess the best thing to do would be to check if my simulations
   agree with Guy's prediction in a low entropy limit, or in general
   to compare for eg the 4000-pattern case (might help to consider
   short time windows and spike counting metric). Otherwise, it's
   going to be impossible to do all of the required simulations.
8. One thing I still need to do is to implement Guy's connectivity
   rules (or maybe not, in case we want to compare the numerical
   results with the random bipartite graph).
9. what about doing a frequency-response curve of the grc model to
   test the 30ms integration window hypothesis? As an RC circuit, my
   average grc model has a time constant of about 6ms (so no problem
   on that front), and the longest decay time constant in my synaptic
   models is 121ms for the second component of the NMDAr-mediated
   synapse. But this component is less than 1/6th in amplitude than
   the first component, which has a time constant of 13.5ms, so
   overall I might get lucky and get away with considering a 50ms
   integration window.
10. for the time windowing side of things, it might be possible to just
    do the same simulations with synchronised and desynchronised Golgi
    input, and see what effect that has on the rate-coded information
    flow. We don’t expect it to have /any/ effect, but that should help
    address the reviewer’s concerns.
11. It might even be possible, even though probably outside the scope
    of the work, make an /analytical/ rate-code extension to the
    existing work. Moreno-Bote et al (2006) estimate the firing rate
    and the Fano factor for the spike count in the case of a leaky
    integrate-and-fire neuron with Poisson input and synaptic time
    constants longer than the membrane time constant. This could in
    theory enable us to calculate mean and variance for the spike count
    of a neuron with any given input, and so, by hypothesising a
    Gaussian distribution in spike count, the full conditional
    probability distribution of the spike counts for all the cells in a
    network given the input rates and the connectivity matrix. from
    here one should be able to calculate the MI, as this would look
    like some sort of multidimensional Gaussian channel. This is
    analogous to what Guy did for the explicit calculation of entropy
    in the "small" case with 4000 patterns. Anyway, this approach would
    break down near the refractory limit.
12. How are we going to deal with the effect of STP in my GoC->GrC
    synaptic model? One could hardcode a dependency of the Golgi
    firing rate on the p(MF) parameter, but because of depression, the
    time-averaged amount of inhibitory conductance doesn't scale
    linearly with the Golgi firing rate. Initially we will just use
    a varying tonic inhibition as a thresholding mechanism, and once
    we are suer we can get that working we will move on to using
    inhibitory synaptic input. Note that the levels of tonic
    conductance will be tuned depending on their thresholding effect,
    and not on their biological realism. This will probably mean
    choosing unrealistically high levels of inhibition.
*** Outcomes of the meeting
We are aiming for a set of punctual comparisons (not parameter sweeps)
between different network configurations. Initially, we are aiming to
show that we can reproduce some of the main results in the study with
a spiking network model with detailed synaptic connectivity. In other
words, for example, "show that the network with 4 dendrites is
significantly better than the network with 10 dendrites under
appropriate conditions". Because of the high computational costs
involved, this is more akin to what one would do experimentally than
to a numerical extension of the analytical results.
- network size: 500 GrCs
- use fixed tonic inhibition, set to an appropriate level (possibly
  the "natural" level of ~400pS).
- use Jason's average GrC model
- use my mf->GrC model built from Jason's data
- 1000 patterns
- 50 repetitions per pattern
- simulation time as short as possible compatibly with the duration of
  the integration window (hypothesised to be ~30ms in the paper,
  probably somewhere between that and 120ms, which is the longest time
  constant present in the synaptic mechanisms)
- number of dendrites d=4 and d=10
- use spatially correlated patterns. This should help in making the
  system more susceptible to variations in its parameters by making it
  harder to discriminate the patterns. Actually one could do both:
  correlated and uncorrelated patterns, and show the difference.
- use a pure rate code (rate in, rate out).
If there is time, we could add the following features:
- modulate sinusoidally input rate in time
- use the MUVR metric to cluster output 
**** DONE First assessment of the time involved in running simulations and analyses
     CLOSED: [2013-11-22 Fri 10:24]
***** DONE Back-of-the-envelope estimation of computational cost of simulations
      CLOSED: [2013-11-19 Wed 01:10]
This is with respect to my old runs back in spring 2012, which if I
remember correctly took about a week on matlem with 180 processors
(even though I might be overestimating this time if the batches of
jobs I was running were larger. Anyway, the values listed here were
for a simulation run size that I considered "typical", small enough to
be rerun if something went wrong).
|-------------------------+--------+--------+--------|
| dimension               | from   | to     | factor |
|-------------------------+--------+--------+--------|
| network size (GrCs)     | 60     | 500    | 8.3    |
| patterns                | 20     | 1000   | 50     |
| repetitions per pattern | 200    | 50     | 1/4    |
| simulation length       | 300ms  | 50ms   | 1/6    |
| parameter space points  | > 80   | < 8    | < 1/10 |
| 1/processors used       | ~1/180 | ~1/220 | 0.8    |
|-------------------------+--------+--------+--------|
| total                   |        |        | < 1.4  |
This of course assumes everything scales linearly and doesn't account
for overheads associated to running a larger number of smaller
simulations and having more complex synaptic models, but it is
encouraging, at least from the point of view of running the
simulations.
***** DONE Test how many cores I can actually use on the cluster
      CLOSED: [2013-11-20 Wed 10:03]
      - CLOSING NOTE [2013-11-20 Wed 10:03] \\
        It seems I can get about *238 cores*.
***** DONE see if there's any leftover issue with pymuvr
      CLOSED: [2013-11-22 Fri 10:17]
      - CLOSING NOTE [2013-11-22 Fri 10:17] \\
	Issue fixed in [[magit:~/phd/code/network/src/::commit@53ce7fb][commit #53ce7fb]]. This was just a matter of
        fixing a leftover bug in the integration with pymuvr.
***** DONE See if I can run the current version of the pipeline (including analysis) out-of-the-box for a problem of the order of magnitude we're interested in.
      CLOSED: [2013-11-22 Fri 10:20]
      - CLOSING NOTE [2013-11-22 Fri 10:20] \\
	The pipeline seems to work, but we'd be better off using a pure rate
	code for better performance.
***** Conclusion
My quick estimate above seems to hold for the simulation, but I think
that for the analysis we need to use a simpler decoder.
** Model-related and scientific choices that need to be made
*** TODO frequency response of the GrC model to synaptic stimulation
    I have to check if the 30ms integration time window hypothesis is
    justified, and adjust the length of the simulations accordingly.
    It looks like a good way of justifying this would be by looking at
    the autocorrelation function of the spike train produced by the
    cell model under Poisson stimulation at different rates. This
    actually shows that, for stimulation rates high enough to give a
    decent estimate of the ACF with a simulation 60s long, the
    autocorrelation seems to drop to its asymptotic value (the average
    firing rate) after about 50ms (see figures below, and note that
    the reported stimulation rate is for each of 4 MFs).
    
    Another interesing phenomenon can be seen in this experiment: as
    reported in Moreno-Bote and Parga 2006, there are two regimes
    depending on whether the average membrane depolarization is above
    or below threshold. Below threshold firing is driven by
    fluctuations and input spike bunching, while above threshold
    the cell tend to fire regularly and fluctuations only add noise to
    this process. This is reflected in a different structure of the
    ACF in the two cases.

    [[file:fig/acf_40Hz.png][file:fig/acf_40Hz.png]]
    [[file:fig/acf_40Hz_detail.png][file:fig/acf_40Hz_detail.png]]
    [[file:fig/acf_100Hz.png][file:fig/acf_100Hz.png]]
    [[file:fig/acf_100Hz_detail.png]]
    [[file:fig/acf_160Hz_detail.png]]

**** TODO Have a look at how Guy explains the 30ms assumption in the paper.
**** DONE Look at lower stimulation frequencies and longer simulations, to make sure these observations do not depend on the rates chosen.
     CLOSED: [2013-11-28 Thu 18:45]
     - CLOSING NOTE [2013-11-28 Thu 18:45] \\
       It's fine. Even with rates close to the cell threshold the
       autocorrelation falls to the 'uncorrelated baseline' in less than
       about 40ms.
*** output decoder: pure rate code vs spike metric
    The tradeoff between performance and added usefulness seems in
    favour of using a pure rate code, at least in the first stage as
    we use a rate-coded input.
*** DONE inhibition: decide on the (fixed) level of tonic GABA conductance
    CLOSED: [2013-11-27 Wed 12:26]
    - CLOSING NOTE [2013-11-27 Wed 12:26] \\
      The experimental value of 0.438nS seems the best option.
*** DONE mf -> grc synapses: decide if total conductance should scale inversely with the number of dendrites.
    CLOSED: [2013-11-28 Thu 18:44]
    - CLOSING NOTE [2013-11-28 Thu 18:44] \\
      No, it shouldn't change, otherwise altering the number of dendrites in
      the spiking model would have a very different meaning than in th
      binary model.
*** DONE Make a table of all the parameters in the model, noting where each of them comes from.
    CLOSED: [2013-11-28 Thu 16:21]
    - CLOSING NOTE [2013-11-28 Thu 16:21] \\
      This is ready to be sent to Angus.
    |----------------+----------------------------------+-----------+-----------------------|
    | model          | parameter                        | value     | source                |
    |----------------+----------------------------------+-----------+-----------------------|
    | Grc IaF        | leak reversal potential          | -79.9 mV  | Schwartz2012          |
    |                | leak conductance                 | 1.06 nS   |                       |
    |                | membrane capacitance             | 3.22 pF   |                       |
    |                | spike threshold                  | -40 mV    |                       |
    |                | reset potential                  | -63 mV    |                       |
    |                | refractory time                  | 2 ms      |                       |
    |----------------+----------------------------------+-----------+-----------------------|
    | GABA (tonic)   | reversal potential               | -79.1 mV  | Seja2012              |
    |                | conductance                      | 0.438 nS  | Rothman2009           |
    |----------------+----------------------------------+-----------+-----------------------|
    | AMPA direct    | reversal potential               | 0 mV      | Rothman2009 (new fit) |
    |                | amplitude 1                      | 3.724 nS  |                       |
    |                | amplitude 2                      | 0.3033 nS |                       |
    |                | rise time                        | 0.3274 ms |                       |
    |                | decay time 1                     | 0.3351 ms |                       |
    |                | decay time 2                     | 1.651 ms  |                       |
    |                | plasticity: initial release prob | 0.1249    |                       |
    |                | plasticity: depr. recovery time  | 131 ms    |                       |
    |----------------+----------------------------------+-----------+-----------------------|
    | AMPA spillover | reversal potential               | 0 mV      | Rothman2009 (new fit) |
    |                | amplitude 1                      | 0.2487 nS |                       |
    |                | amplitude 2                      | 0.2799 nS |                       |
    |                | amplitude 3                      | 0.1268 nS |                       |
    |                | rise time                        | 0.5548 ms |                       |
    |                | decay time 1                     | 0.4 ms    |                       |
    |                | decay time 2                     | 4.899 ms  |                       |
    |                | decay time 3                     | 43.1 ms   |                       |
    |                | plasticity: initial release prob | 0.2792    |                       |
    |                | plasticity: depr. recovery time  | 14.85 ms  |                       |
    |----------------+----------------------------------+-----------+-----------------------|
    | NMDA           | reversal potential               | 0 mV      | Rothman2009 (new fit) |
    |                | amplitude 1                      | 17 nS     | Schwartz2012          |
    |                | amplitude 2                      | 2.645 nS  | Schwartz2012          |
    |                | rise time                        | 0.8647 ms | Rothman2009 (new fit) |
    |                | decay time 1                     | 13.52 ms  | Rothman2009 (new fit) |
    |                | decay time 2                     | 121.9 ms  | Rothman2009 (new fit) |
    |                | plasticity: release probability  | 0.0322    | Rothman2009 (new fit) |
    |                | plasticity: depr. recovery time  | 236.1 ms  | Rothman2009 (new fit) |
    |                | plasticity: pot. recovery time   | 6.394 ms  | Rothman2009 (new fit) |
    |                | Mg2+ block: Z                    | 2         | Schwartz2012          |
    |                | Mg2+ block: T                    | 35 degC   | Schwartz2012          |
    |                | Mg2+ block: [Mg2+]               | 1 mM      | Schwartz2012          |
    |                | Mg2+ block: delta_bind           | 0.35      | Schwartz2012          |
    |                | Mg2+ block: delta_perm           | 0.53      | Schwartz2012          |
    |                | Mg2+ block: C1                   | 2.07 mM   | Schwartz2012          |
    |                | Mg2+ block: C2                   | 0.015 mM  | Schwartz2012          |
    |----------------+----------------------------------+-----------+-----------------------|
    | MF terminal    | refractory time                  | 1 ms      | Schwartz2012          |
**** DONE find an appropriate reference for the upper limit on MF firing rate
     CLOSED: [2013-11-29 Fri 19:05]
     - CLOSING NOTE [2013-11-29 Fri 19:05] \\
       Rancz2007 and references 8-11 therein should be enough.
*** TODO specify a form of input pattern correlation
*** TODO finalise a choice for the sparsity measure
    The best candidates at the moment are:
     - a properly normalised version of the Treves-Rolls sparsity
       measure, or something along those lines; for example the Hoyer
       sparsity measure as described in Hurley2009
     - what Willmore and Tolhurst (2001) call 'activity
       sparseness'.
*** Plan B: is there an easy way of integrating synaptic noise in our analysis?
** Technical upgrades needed 
*** Simulation features
**** DONE Check that GrC and synaptic model actually correspond to the latest version from Rothman and Piasini 2013.
     CLOSED: [2013-11-22 Fri 13:10]
**** DONE Update the GABA reversal potential with the value reported in Seja2012.
     CLOSED: [2013-11-22 Fri 17:03]
     - CLOSING NOTE [2013-11-22 Fri 17:03] \\
       Updated the value in the GrC model file in the original OSB repo and
       in the if_gl neuroConstruct project.
**** DONE Change the tonic inhibition from being a current to being a conductance.
     CLOSED: [2013-11-22 Fri 16:59]
     - CLOSING NOTE [2013-11-22 Fri 16:59] \\
       Tonic GABA has been added as a conductance. Its value is fixed,
       though, and at the moment the "bias" parameter still controls an
       inhibitory current.
**** DONE If the simulation time is short, should the variables in the synaptic plasticity mechanisms be initialised at their steady state value for the given stimulation frequency?
     CLOSED: [2013-11-27 Wed 12:17]
     - CLOSING NOTE [2013-11-27 Wed 12:17] \\
       No, we'd better keep simulation time a bit longer (say 200ms) and only
       analyse the final window of the simulation (50, 100 or 150ms).
**** DONE Make a simple f-f curve for the GrC model with 4 dendrites.
     CLOSED: [2013-11-29 Fri 14:21]
     - CLOSING NOTE [2013-11-29 Fri 14:21] \\
       #+CAPTION: Comparison of the f-f curve for my model in LEMS and Jason's model cell number 156. The model receives 4 independent Poisson MF stimuli, plus tonic inhibition. Cell number 156 has been chosen because it's the one whose parameters are closer to the average of the population (ie it's an approximation of 'the average model'). Generated using [[magit:~/phd/nC_projects/GranCellRothmanIf/::commit@4671576][commit #4671576]] in the granule cell model repo.
       #+NAME: fig:rate_IO
       [[file:fig/rate_IO.png]]
**** DONE Make a comparison between my f-f curve and what Jason had in Schwartz2012.
     CLOSED: [2013-12-02 Mon 14:57]
     - CLOSING NOTE [2013-12-02 Mon 14:57] \\
       Figure [[fig:rate_IO]] shows that my model tends to fire a bit more
       than Jason's. This was expected, and (given that the cell
       models are very similar in the two cases) mostly depends on the
       time-averaged NMDA component of my synaptic model being
       somewhat larger. This, in turn, depends on the fact that both
       me and Jason normalised the NMDA waveform at -60mV to have the
       same peak amplitude as the AMPA component, with the two NMDA
       models having a different base waveform shape. Specifically, my
       waveform is somewhat less peaked than Jason's, so this results
       in a larger time-averaged conductance in my mode. Anyway, as
       can be seen in the figure below, the value for the
       time-averaged NMDA to AMPA ratio remains within the biological
       range as published in Schwartz2012.
       #+NAME: fig:NMDA_to_AMPA_ratio
       #+ATTR_HTML: :width 100%
       [[file:fig/NMDA_to_AMPA_ratio.png][file:fig/NMDA_to_AMPA_ratio.png]]
**** DONE +Implement Guy's network connection algorithm+
     CLOSED: [2013-12-02 Mon 10:30]
     - CLOSING NOTE [2013-12-02 Mon 10:30] \\
       No need for this; Guy is going to send me a bunch of network
       realisations exported from his code, and I'll have to import them at
       network creation time.
     Message from Guy:
     #+BEGIN_QUOTE
     Unless you have already solved it I would suggest an
     implementation such as the following

     i) generate glomerular degree distribution (that of a binomial)

     ii) for each (randomly chosen) connection chose a granule cell to
     connect to subject to the constraints:

     connection length as close as possible to 'x' (we used 15 microns I believe)
     granule cell would not have more than 'd' connections
     the connection does not repeat with a granule cell to which the glom. is already connectioned
     the connection would not cause a single mossy fibre to be connection to the same granule cell
     [The last one is a bit of an artificial one that had to be introduced because for high number of connections, the resampling because an problem since it diminishes the number
     of independent inputs to the network, which create conceptual issues]
       
     The above also assumes that all cells and connections are
     confined to the same sphere (connections do not penetrate the
     sphere holding the GRCs).

     The problem I am having is that I also definately had to create a
     method to shuffle some connections about with the aim of
     minimising the mean squared deviation from the target dendrite
     lenght. I think it might be that the above created a weird
     dendrite length distribution (i.e. not central). But I am not
     sure and I cant look at my Mathematica notebooks right now.

     So try the above. You might find it works in which case no
     problem. If there are any problems however, let me know because I
     will probably remember it at that stage!
     #+END_QUOTE
**** DONE [1/1] Reduce maximum number of files used by a complete run through of the workflow.
     CLOSED: [2013-12-05 Thu 11:13]
     - CLOSING NOTE [2013-12-05 Thu 11:13] \\
       Improved garbage collection in [[magit:~/phd/code/network/src/::commit@78beb48][commit #78beb48]]. Let's see if
       this is enough.
     - [X] Make sure nC garbage gets cleaned up before a simulation
       job exits.
**** TODO Use ~bias~ parameter to control tonic GABA.
**** DONE Fix csv export of glomerular positions in Guy's mathematica notebook.
     CLOSED: [2013-12-08 Sun 19:03]
     - CLOSING NOTE [2013-12-08 Sun 19:03] \\
       Done. This required changing his Mathematica library to add a function
       to compute an adjacency matrix for the network composed by glomeruli
       and granules (as opposed to the network composed by mossy fibres and
       granules), and updating the network generation notebook accordingly.
     It seems like glomerular positions are expressed as a list of
     lists of 3d points, but the length of the lists of points is not
     constant. Also, the graphml file seems to contain a glomerular
     node for each list, and not for each point. What are the extra 3d
     points for? what's the correspondence between nodes in the graph
     and spatial coordinates? Maybe this has something to do with
     having the glomeruli grouped by the MFs to which they belong, and
     then keeping only one glomerulus per MF at some point downstream
     in the code during network building.
**** DONE Implement importer for text-based network descriptions.
     CLOSED: [2013-12-08 Sun 19:05]
     - CLOSING NOTE [2013-12-08 Sun 19:05] \\
       Done in [[magit:~/phd/code/network/src/::commit@63aacda][commit #63aacda]], and added a script in [[magit:~/phd/code/network/src/::commit@ebe4d78][commit #ebe4d78]]
       which takes care of setting up the data directories and to copy
       the graphml files in place for a predefined set of network
       structure configurations. Shown below in figure
       [[fig:tissue_model_3d_view]] is a 3D view (made with the
       [[file:scripts/visualise_network.py::filename%20%3D%20'/home/ucbtepi/code/network/data/network_structures/gd4/GCLconnectivity_full.graphml'][mayavi-based network visualisation script]]) and in figure
       [[fig:tissue_model_2d_view]] a (x, y) projection (made with [[https://gephi.org][Gephi]])
       of an instantiation of the network with 4 dendrites. Figure
       [[fig:tissue_model_distance_from_gr]] shows a map of the number of
       degrees of separation between one granule node (in red) and the
       other nodes. A correlation between euclidean and separation
       distance is evident.
       #+CAPTION: three-dimensional view of a network exported from the Mathematica toolbox.
       #+NAME: fig:tissue_model_3d_view
       #+ATTR_HTML: :width 60%
       [[file:fig/tissue_model_3d_gd4.png][file:fig/tissue_model_3d_gd4.png]]

       #+CAPTION: (x, y) projection of an exported network. Red: glomeruli; light blue: granule cells.
       #+NAME: fig:tissue_model_2d_view
       #+ATTR_HTML: :width 60%
       [[file:fig/tissue_model_gd4.png][file:fig/tissue_model_gd4.png]]

       #+CAPTION: color-coded map of length of the shortest path between the granule cell in red and all the other cells in the network. Light colours mean longer distances.
       #+NAME: fig:tissue_model_distance_from_gr
       #+ATTR_HTML: :width 60%
       [[file:fig/tissue_model_distance_from_gr_gd4.png][file:fig/tissue_model_distance_from_gr_gd4.png]]
       
**** TODO Implement input correlations.
     In the paper, an input frequncy range of about 10-50Hz was
     mentioned when justifying the 30ms integration window
     hypothesis. My input patterns should live within that range as
     well.
*** Analysis features
**** DONE Implement pure rate-code decoder with k-means clustering.
     CLOSED: [2013-11-22 Fri 10:54]
     - CLOSING NOTE [2013-11-22 Fri 10:54] \\
       Done in [[magit:~/phd/code/network/src/::commit@1e7745c][commit #1e7745c]].
**** DONE check if analysis workflow can handle a problem of the scale we're interested in
     CLOSED: [2013-11-25 Mon 16:00]
     - CLOSING NOTE [2013-11-25 Mon 16:00] \\
       The whole workflow can run overnight for a single parameter space
       point, 1000 patterns, 50 repetitions, 150ms long simulations, and
       rate-coded k-means based decoding.
**** DONE [2/2] Implement sparsity measure.
     CLOSED: [2013-12-03 Tue 16:22]
     - CLOSING NOTE [2013-12-03 Tue 16:22] \\
       Implemented both 'activity' and 'hoyer' sparseness measures in [[magit:~/phd/code/network/src/::commit@0577178][commit #0577178]].
     - [X] Implement measure candidates, so that I can play around
       with them while we decide on which ones to use.
     - [X] Add sparseness estimation to analysis workflow.
**** DONE Take info on number of cells from archive metadata and not from size of spike tables.
     CLOSED: [2013-12-08 Sun 19:08]
     - CLOSING NOTE [2013-12-08 Sun 19:08] \\
       Done in [[magit:~/phd/code/network/src/::commit@d239378][commit #d239378]].
**** TODO Add dimension to analysis parameter space to represent integration window length.
**** DONE Remove saving of all rates to results archives before running with big network
     CLOSED: [2013-12-08 Sun 19:13]
     - CLOSING NOTE [2013-12-08 Sun 19:13] \\
       Done in [[magit:~/phd/code/network/src/::commit@8564526][commit #8564526]].
* Questions to be addressed with the model after we're sure it works
** Is the network with d=4 any better than the one with d=10 at information transmission or sparsification?
   My guess is that, unless our choice of input correlations make a
   massive difference, the d=10 network will be just as good as the
   d=4 network at transmitting information, mainly because we are
   operating with such a small number of patterns and so far away from
   channel capacity. What /could/ be different, though, is the
   sparsification: in that case, if the d=10 network sparsifies less,
   we can then ask: /what is the required amount of tonic inhibition
   to bring sparsification back to where it was with d=4? and what
   happens to information transmission if we set inhibition to that
   value?/ This would also offer a good way of formulating an
   experimentally testable hypothesis: if information transmission is
   still good even in this heavily inhibited case, then this is the
   homeostatic compensation we expect to arise in a mutant mouse with
   large d; otherwise, we expect to see some other mechanism coming
   into play, but still something that would tend towards allowing
   complete information transmission with a sparse code (for example,
   conformational changes in the granule cell's ion channels or a
   decrease in the amount of neurotransmitter released at the mossy
   fibre synapse). Or the mutant mouse's cerebellum might not function
   as well as the wild type's, of course.
*** Pushing the network to its limits <2013-12-04 Wed>
    We assume that the purpose of the network is to strike a balance
    between information transmission and sparsification. Some of the
    parameters in the simulations are relatively free (like input
    correlations and integration time window), and some can be changed
    but we have an idea of what they should be in a 'normal' network
    (like the amount of tonic inhibition). We want to compare, say,
    the network with 4 dendrites to the network with 20 dendrites.
**** Free parameters (input correlation, length of integration time window)
     We start by estimating, in parameter space, the edge of the
     region where the network with 4 dendrites transmits all of the
     information. In practice, this means finding the maximum amount
     of correlations in the inputs or the shortest integration window
     we can use before we start losing information. We then see how
     the network with 20 dendrites performs in these "extreme"
     conditions: our guess is that it will transmit less information
     for strongly correlated inputs and it will sparsify
     (proportionately) less for short integration time windows. My
     idea, though, by looking at the f-f curve in figure [[fig:rate_IO]]
     and considering that our input rates won't go above 50Hz, is that
     _at about 100ms we are already fairly close to the lower limit
     for the integration window in the network with 4 dendrites_, as
     already with 4 30Hz inputs we are below the 10Hz firing threshold
     for detection of a single spike in such a time interval.
**** Tonic inhibition
     Tonic inhibition is not really a free parameter of our model, and
     also has a very direct and obvious effect on sparsification. My
     proposal is that we treat it as an example of a possible
     homeostatic mechanism that could be used by the organism to
     balance the extra excitation the GCL would get if the number of
     dendrites was larger. We can then proceed as follows: we look at
     the network in 'normal' conditions and we note the MI and the
     sparsification, then we increase the number of dendrites from 4
     to 20 and we estimate again these two measures. Presumably, MI
     would be the same (if it's not, all the better) but there will be
     less sparsification. We could then increase inhibition to a level
     that will bring sparsification back to what we had with 4
     dendrites, and observe if there is any decrease in transmitted
     information. If that's the case, we can conclude that it's not
     possible to achieve the same optimal tradeoff between information
     and sparsity in a network with many dendrites without resorting
     to some more complicated homeostatic mechanism.

*** 1/r benchmark as mentioned in the discussion?


*** TODO Once I have a sparsity measure, make some exploratory simulations (even with the random graph model for the network) to have a rough idea of the behaviour of the MI and the sparsity for the cases with, say, 4 and 20 dendrites.
    <2013-12-03 Tue> Preliminary simulations indicate that, at least
    for the random bipartite graph and uncorrelated inputs, and for an
    initial fraction of active mossy fibers of 0.5, the results on
    sparsification are what we expect:
    | dendrites | in sp (activity) | out sp (activity) | in sp (hoyer) | out sp (hoyer) |
    |-----------+------------------+-------------------+---------------+----------------|
    |         3 |              0.5 |              0.85 |          0.27 |           0.61 |
    |         9 |              0.5 |              0.14 |          0.26 |           0.09 |
    We can also note that, at least in this case, the /activity
    sparseness/ measure seems to reflect better our intuition (I
    wonder if the Treves-Rolls measure would perform better than the
    Hoyer measure, here, the main difference between the two being a
    square root operation). I'm running now a parameter sweep over
    ~n_grc_dend~ and ~active_mf_fraction~ with a relatively small
    number of simulated patterns (100) to understand if this is a
    fluke or not, and more generally to compare the behaviour of the
    two sparseness measures across a wider range of conditions.
    
    <2013-12-05 Thu> The results from the parameter sweep look
    encouraging in terms of behaviour of the 'activity' sparseness. It
    looks like a (random) network with 4 dendrites sparsifies all
    the time, one with 6 dendrites keeps the sparseness roughly the
    same between input and output, and networks with 8 or 10 dendrites
    always amplify.
